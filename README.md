[_metadata_:author]:- "Emil Niclas Meyer-Hansen"
[_metadata_:date]:- "2026-02-07"
[_metadata_:tags]:- "markdown metadata"
# Conceptualizing Surprise with the Jensen‚ÄìShannon distance: A Bayesian Information-Theoretic Approach for the Social Sciences

*Emil Niclas Meyer-Hansen*

2026-02-07

A formal (re)conceptualization of surprise is developed by the author for the social sciences in the research paper *Conceptualizing Surprise with the Jensen‚ÄìShannon distance: A Bayesian Information-Theoretic approach for the Social Sciences* (see the Abstract below). With still unfinished sections, the paper is currently very much a work-in-progress. It subscribes to the [*open science standard*](https://www.cos.io/open-science), is partly licensed under the CC BY 4.0 and partly under the GPL-3.0 (see the License below), and it is made freely available in PDF-format.

## Table of Contents
- [Abstract](#abstract)
- [FAQ](#faq)
- [Changelog](#changelog)
- [License](#license)
- [Citation](#citation)

<a id="abstract"></a>
## Abstract

Generally in the social sciences, results are informally deemed surprising if their associated ùëù-value is sufficiently small. This implicit interpretation, however, is both conceptually and mathematically inappropriate, and such misuse of the *p*-value can lead to erroneous conclusions about the novelty of results. To solve that issue, this paper builds on Bayesian inference, Information theory, and considerations specific to the social sciences, to argue for the adoption of a more appropriate conceptualization of surprise as the relative entropy between prior and posterior knowledge. Novel to the social sciences, this formal conceptualization enables researchers to appropriately measure surprise as the Jensen-Shannon distance, for which the paper contributes with easily implementable software and a demonstration of its use in relation to empirical data.[¬©](#license)

**Keywords**: *SurpriseÕæ NoveltyÕæ Jensen-Shannon distanceÕæ JS distanceÕæ Jensen-Shannon divergenceÕæ JS divergenceÕæ Relative entropyÕæ Kullback-Leibler divergenceÕæ KL divergenceÕæ DistanceÕæ DivergenceÕæ DissimilarityÕæ Differential entropyÕæ Entropy; p-valueÕæ S-valueÕæ Information theoryÕæ Bayesian inference*

<a id="faq"></a>
## FAQ
- Why are there errors (e.g., mispellings) in the paper? The research paper is currently missing sections because it is a work-in-progress. These errors will be fixed on an ongoing basis.
- Why is there missing sections in the paper? The research paper is currently missing sections because it is a work-in-progress. The author expects the missing sections to be presentable by mid February.

<a id="changelog"></a>
## Changelog
- **2026-02-07 15:20 CEST**
  - [Version 2026-02-07-15-20] - Working Paper (Minor revisions).
     - Minor revisions (e.g., added missing words, corrected mispellings)
     - Fixed issue in JS function that prevented the user-specified base being used for the logarithmic function.
- **2026-02-01 13:58 CEST**
  - [Version 2026-02-01-13-58] - Working Paper (Revised Release).
     - Added content for the 'Demonstration'-section.
     - Improved existing sections.
- **2026-01-14 15:45 CEST**
  - [Version 2026-01-14-15-45] - Working Paper (Initial Release).

<a id="license"></a>
## License (Addendum)

Except where otherwise indicated, all contents of this document and associated files are licensed under the *Creative Commons Attribution 4.0 International License* ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)). All software, including but *not* necessarily limited to, source code, executable code, code snippets, code chunks, algorithms, and/or scripts, attributable to this document and/or any of its associated files are expressly excluded from the foregoing license, and unless otherwise indicated, are instead licensed under the *GNU General Public License, version 3* ([GPL-3.0](https://www.gnu.org/licenses/gpl-3.0.html)). By engaging with this document and/or any associated files, which include, but are *not* necessarily limited to, downloading, using, viewing, and/or distributing any of them, in parts or whole, you agree to comply with the applicable license terms for the respective content types.

<a id="citation"></a>
## Citation

Building on Bayesian inference, Information theory, and similar conceptualizations made for other scientific disciplines, this formal conceptualization of surprise as the relative entropy between prior and posterior knowledge is an original (re)conceptualization by Emil Niclas Meyer-Hansen specifically for the social sciences, conceived as part of the research paper associated with this project. For correspondence, contact the author via email: [emil098meyerhansen@gmail.com](mailto:emil098meyerhansen@gmail.com)

Please, if you use, refer to, modify, and/or continue the development of this formal conceptualization of surprise for the social sciences, provide proper reference and citation to its founding author. An example of proper citation is provided below:
```
Meyer-Hansen, E. N. (2026): 'Conceptualizing Surprise with the Jensen‚ÄìShannon distance: A Bayesian Information-Theoretic approach for the Social Sciences', Open Science Framework, Working paper (v2026-02-07-15-20). DOI: [10.17605/OSF.IO/GQ6C8](https://doi.org/10.17605/OSF.IO/GQ6C8)
```

For LaTeX users, a BibTeX entry is provided below:
```
@unpublished{,
  title = {Conceptualizing Surprise with the Jensen‚ÄìShannon distance: A Bayesian Information-Theoretic approach for the Social Sciences},
  author = {Emil Niclas Meyer-Hansen},
  publisher = {Open Science Framework},
  year = {2025},
  doi = {10.17605/OSF.IO/GQ6C8},
  pubstate = {Working Paper (v2026-02-07-15-20)}
}
```
